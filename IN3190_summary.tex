\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,graphicx,varioref,verbatim,amsfonts,geometry}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage{mathtools}
\usepackage{textcomp}
\usepackage[english, norsk]{babel}
\usepackage{wrapfig}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{lastpage}
\usepackage{graphicx}
\usepackage{matlab-prettifier}
\usepackage{filecontents}
\usepackage{underscore}
\usepackage{makecell}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{siunitx}
\usepackage{mathrsfs}

\pagestyle{fancy}


%% Double underline 
\def\dubline#1{\underline{\underline{#1}}}

%% Matrix
\newcommand\SmallMatrix[1]{{%
		\arraycolsep=0.3\arraycolsep\ensuremath{\begin{pmatrix}#1\end{pmatrix}}}}
	
%% Codeinsert
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\lstset{
	backgroundcolor=\color{lbcolor},
	tabsize=4,
	rulecolor=,
	language=python,
	basicstyle=\scriptsize,
	upquote=true,
	aboveskip={1.5\baselineskip},
	columns=fixed,
	numbers=left,
	showstringspaces=false,      
	extendedchars=true,
	breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	frame=single,
	showtabs=false,
	showspaces=false,
	showstringspaces=false,
	identifierstyle=\ttfamily,
	keywordstyle=\color[rgb]{0,0,1},
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\color[rgb]{0.627,0.126,0.941}
}
\newcounter{subproject}
\renewcommand{\thesubproject}{\alph{subproject}}
\newenvironment{subproj}{
	\begin{description}
		\item[\refstepcounter{subproject}(\thesubproject)]
	}{\end{description}}

\begin{document}
\title{IN3190 - Exam Preparation Questions H23\\
{\Large University of Oslo}}
\author{Daniel Tran \& Erik Røset}
\date{November 2023}
\fancyhead[L]{Should know to exam H23 - IN3190}
\fancyhead[C]{UiO}
\fancyhead[R]{Daniel Tran \& Erik Røset}
\cfoot{\thepage\ of \pageref{LastPage}}
\maketitle
\hypersetup{linkcolor=black}
\tableofcontents
\clearpage

\section{Essential Topics}
\subsection{Discrete time}
\subsubsection{Sine, cosine and exponential functions}

    Mathematical formula that establishes the fundamental relationship between the trigonometric functions and the complex exponential function. \newline
    Euler's formula states that for any real number $x$:
    \begin{equation}
        e^{ix} = \cos(x) + i\sin(x)
    \end{equation}
    When $x = \pi$, Euler's formula yields Euler's identity:
    \begin{equation}
        e^{i\pi} + 1 = 0
    \end{equation}
\subsubsection{Elementary discrete signals}
\paragraph{Unit impulse} %Spør gruppelærer om plot fra forelesning
Also known as the dirac delta function. Shifted to the left when its negative and to the right when its positive. Can be used to find H(n) in a system. If the input $x(n) = \delta(n)$, then the output $y(n) = h(n)$.
\begin{equation}
    \delta[n] = \begin{cases}
        1, & n = 0 \\
        0, & n \neq 0
    \end{cases}
    \hspace*{20pt}\text{may also be written as:}
    \hspace*{20pt} \delta[n] = u[n] - u[n-1]
\end{equation}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Discrete time/unit_impulse.png}
    \caption{Unit impulse}
    \label{fig:unit_impulse}
\end{figure}

\paragraph{Step function}
Also known as unit step, unit step function or heaviside function. The value which is zero for negative arguments and one for positive arguments.
\begin{equation}
    u[n] = \begin{cases}
        1, & n \geq 0 \\
        0, & n < 0
    \end{cases}
    \hspace*{20pt}\text{may also be written as:}
    \hspace*{20pt} u[n] = \sum_{k=0}^{\infty} \delta[n-k]
\end{equation}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Discrete time/unit_step_function.png}
    \caption{Unit step function}
    \label{fig:unit_step}
\end{figure}

\newpage
\paragraph{Ramp function}
Also known as the unit-ramp or unit ramp function. Graph shaped like a ramp.
\begin{equation}
u_r[n] = \begin{cases}
    n, & n \geq 0 \\
    0, & n < 0
\end{cases}
\end{equation}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Discrete time/unit_ramp_function.png}
    \caption{Unit ramp function}
    \label{fig:unit_ramp}
\end{figure}

\paragraph{Periodic sequences} 
$x[n]$ is periodic if and only if $x[n] = x[n+N]$
\begin{itemize}
    \item \textbf{Fundamental period}: 
    \newline Smallest positive integer N which fulfills the relation above
    \item \textbf{Sinusoidal sequences}: 
    \newline $x[n] = A\cos(\omega_0n + \phi)$, where 
    \begin{itemize}
        \item $A$ is amplitude,
        \item $\omega_0$ is the angular frequency
        \item and $\phi$ is the phase of $x[n]$.
    \end{itemize} 
    $x[n]$ is periodic if and only if $\omega_0N = 2\pi k$, for N and k as positive integers.
\end{itemize}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Discrete time/sinusoidal_sequences.png}
    \caption{Sinusoidal sequences}
    \label{fig:Sinusoidal_sequence}
\end{figure}

\subsubsection{LTI systems, including characteristics via the transformation between input and output}
\paragraph{Linearity:} The relationship between the input $x(t)$ and the output $y(t)$, both being regarded as functions. If $a$ is a constant then the system output to $ax(t)$ is $ay(t)$. Linear system if and only if it $H\{\cdot\}$ is both additive and homogeneous, in other words: If it fulfills the superposition principle. That is:
\begin{equation}
    H\{ax_1(t)+bx_x(t)\} = aH\{x_1(t)\} + bH\{x_2(t)\}
\end{equation}

\paragraph{Time-invariance:} 
A linear system is time-invariant or shift-invariant means that wether we apply an input to the system now or T seconds from now, the output will be identical except for a time delay of S seconds. In other words, if $y(t)$ is the output of a system with a input $x(t)$, then the output of the system with input $x(t-T)$ is $y(t-T)$. The system is invariant becasue the output does not depend on the particular time at which the input is applied.
\\
\\
The fundamental result in LTI system theory is that any LTI system can be characterized entirely by a single function called the system's impulse response $h(t)$. The output of the system $y(t)$ is simply the convolution of the input to the system $x(t)$ with the systems's impulse response $h(t)$.
\\
\\
LTI system can also be characterized in the frequency domain by the system's transfer function $H(s)$, which is the Laplace transform of the system's impulse response $h(t)$. Or Z-transform in the case of discrete time systems.
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Discrete time/LTI_time_freq_domain.png}
    \caption{Relationship between the time domain and the frequency domain}
    \label{fig:lti_relationship}
\end{figure}

\subsubsection{Difference equations for system description / LTI systems written on general summation notation with coefficients and time offsets} %Ask why M
General description of relationship between input x[n] and output y[n] of a LTI system in the time domain (n domain): (M amount of zeroes) (N amount of poles)
\begin{equation}
    y[n] = -\sum_{k=1}^{N}a_ky[n-k] + \sum_{k=0}^{M}b_xx[n-k]
\end{equation}
\begin{itemize}
    \item FIR (finite impulse response): all $a_k = 0$
    \item IIR (infinite impulse response): at least one $a_k \neq 0$
\end{itemize}
The value N represent the order of the difference equation and corresponds to the memory of the system being represented. Because this equations relies on past values of the output, in order to compute a numerical solution, certain past outputs, referred to as the initial conditions, must be known.
\\
\textbf{$a_k$} and \textbf{$b_k$} are the coefficients of the difference equation. 
Time offset is represented by the index k.

\subsubsection{Casuality}
In control theory, a causal system is a system where the output depends on past and current inputs, but not future inputs. The idea that the output of function at any time depends only on past and present values of input is defined by the property commonly referred to as casusality. 

\subsubsection{Stability and the related convergence for the DTFT}
A linear system is called BIBO (Bounded-input bounded output) stable if its output will stay bounded for any bounded input. 

\paragraph{Discrete-time signals:} For a rational and discrete time system, the conditon for BIBO stability  is that the ROC (region of convergence) of the z-transform includes the unit circle. When the system is causal, the ROC is the open region outside a circle whose radius is the magnitude of the pole with largest magnitude. Therefore, all poles of the system must be inside the unit circle in the z-plane for BIBO stability. For stable systems, the unit circle is part of ROC. 
\\
\\
Causal stable filters have all poles inside $|z| = 1$.

\subsubsection{FIR / IIR}
\paragraph{FIR:} Finite impulse response. The output from a FIR filter is calculated by the basis of the input samples and the proceding input. A FIR filter is always stable because the values from the output does not effect calculations. 
(Can be thought of as an op-amp without feedback loop)


\paragraph{IIR:} Infinite impulse response. The output from a IIR filter is calculated by the basis of the input samples, the proceding input and the previous output. A IIR filter can be unstable because the values from the output effects the calculations. 
(Can be thought of as an op-amp with feedback loop)

\subsubsection{Convolution}
The resulting signal y(n) represents the amount of overlap between x and h at each point in time, and can be used for a variety of purposes, such as filtering, smoothing, deconvolution, and image processing. For example, if we convolve a signal with a smoothing kernel, we can reduce noise and make the signal smoother.
\\
The summation:
\begin{equation}
    y(n) = \sum_{k=-\infty}^{\infty}x(k)h(n-k) = \sum_{k=-\infty}^{\infty}x(n-k)h(k)
\end{equation}
is the convolution of $x(n)$ and $h(n)$

\paragraph{Properties of convolution:}
\begin{itemize}
    \item \textbf{Commutative:} $x(n) \ast h(n) = h(n) \ast x(n)$
    \item \textbf{Associative:} $x(n) \ast \{h_1(n) \ast h_2(n)\} = \{x(n) \ast h_1(n)\} \ast h_2(n)$
    \item \textbf{Distributive:} $x(n) \ast \{h_1(n) + h_2(n)\} = x(n) \ast h_1(n) + x(n) \ast h_2(n)$
    \\
    where $\ast$ is the convolution operator
    \item Can also use the matrix method to calculate convolution
\end{itemize}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/Discrete time/graphic_conovlution.png}
    \caption{Convolution}
    \label{fig:convolution}
\end{figure}

\subsubsection{Sampling frequency and sampling interval. Digital frequency / physical frequency, as well as
corresponding digital angular frequency / physical angular frequency and how to map between these
given a sampling interval T = 1/FT}

\paragraph{Sampling frequency:}
The sampling frequency or sampling rate, $f_s$ is the average number of samples obtained in one second (samples per second), thus $f_s = 1/T_s$ given in Hz or kHz. To avoid aliasing, the sampling frequency must be greater than twice the maximum frequency of the signal being sampled, according to the Nyquist–Shannon sampling theorem.

\paragraph{Sampling interval:}
The sampling interval or sampling period, $T_s$ is the time between samples, and is equal to the inverse of the sampling frequency $T_s = 1/f_s$. The sampling interval should be chosen small enough to avoid aliasing, but large enough to get a reasonable signal-to-noise ratio.

\paragraph{Digital frequency:}
Digital frequency 

\paragraph{Physical frequency:}


\paragraph{Digital angular frequency:}

\paragraph{Physical angular frequency:}


\subsection{Z-transform}

\subsubsection{Definition and how to derive the various z-transform properties}
\paragraph{Definition:}
The Z-transform(z-domain) is a discrete-time equivalent of the Laplace transform(s-domain). It is a linear transformation of a discrete-time signal which maps a complex-valued discrete-time signal $x[n]$ to a complex-valued function of a complex variable $X(z)$.
\begin{equation}
    X(z) = \mathcal{Z}[x[n]] = \sum_{n=-\infty}^{\infty} x[n]z^{-n}
\end{equation}

\subsubsection*{Propterties}

\paragraph{Linearity:}
If $x[n] \xLeftrightarrow{Z.T} X(z)$ and $y[n] \xLeftrightarrow{Z.T} Y(z)$, then linearity property states that:
\begin{equation}
    ax[n] + by[n] \xLeftrightarrow{Z.T}  aX(z) + bY(z)
\end{equation}

\paragraph{Time shifting:}
If $x[n] \xLeftrightarrow{Z.T} X(z)$, then time shifting property states that:
\begin{equation}
    x[n-m] \xLeftrightarrow{Z.T} z^{-m}X(z)
\end{equation}

\paragraph{Time reversal:}
If $x[n] \xLeftrightarrow{Z.T} X(z)$, then time reversal property states that:
\begin{equation}
    x[-n] \xLeftrightarrow{Z.T} X(z^{-1})
\end{equation}

\paragraph{Scaling in the z-domain:} 
If $x[n] \xLeftrightarrow{Z.T} X(z)$, then scaling property states that:
\begin{equation}
    a^nx[n] \xLeftrightarrow{Z.T} X(a^{-1}z)
\end{equation}
for any constant $a$, real or complex.

\paragraph{Differentiation in the z-domain:}
If $x[n] \xLeftrightarrow{Z.T} X(z)$, then differentiation property states that:
\begin{equation}
    x[n] \xLeftrightarrow{Z.T} \left [-z\frac{dX(z)}{dz} \right ]^k
\end{equation}

\paragraph{Convolution of two sequences:} 
If $x[n] \xLeftrightarrow{Z.T} X(z)$ and $y[n] \xLeftrightarrow{Z.T} Y(z)$, then convolution property states that:
\begin{equation}
    x[n] \ast y[n] \xLeftrightarrow{Z.T} X(z)Y(z)
\end{equation}

\subsubsection{Region of convergence (ROC)}
The region of convergence, known is ROC, is important to understand because it defines the region where the z-transform exist.
\begin{equation}
    ROC = \left \{z:\left|\sum_{n=-\infty}^{\infty}x[n]z^{-n} \right| < \infty \right \}
\end{equation}
\begin{itemize}
    \item ROC cannot contain any poles
    \item From ROC, a system is causal if goes to $\infty$
    \item If the ROC contains the unit circle, then the system is stable.
\end{itemize}

\subsubsection{Poles/zeroes as well as the connection to stability/causality/symmetry/real signals/frequency response/filter type/linear phase etc}



\subsection{Frequency analysis and DFT}

\subsubsection{DTFT and its central properties}

\paragraph{Discrete time Fourier transform (DTFT):} The DTFT of a discrete-time signal $x[n]$ is defined as:
\begin{align}
    \text{Synthesis equation:} \quad & x[n] = \frac{1}{2\pi} \int_{-\pi}^{\pi} X(e^{j\omega})e^{j\omega n} d\omega \\
    \text{Analysis equation:} \quad & X(e^{j\omega}) = \sum_{n=-\infty}^{\infty} x[n]e^{-j\omega n}
\end{align}
\begin{itemize}
    \item $X(e^{j\omega})$ unique ocer the frequency interval ($-\pi,\pi$), or equivalently ($0,2\pi$)
    \item $X(e^{j\omega})$ is periodic with period $2\pi$
\end{itemize}

\begin{figure}[h!]
    \centering
    \begin{tabular}{|c c c|}
        \hline
        Property & Time domain & Frequency domain \\
        \hline
        Notation & $x[n]$ & $X(\omega)$ \\
        & $x_1(n)$ & $X_1(\omega)$ \\
        & $x_2(n)$ & $X_2(\omega)$ \\
        Linearity & $ax_1(n) + bx_2(n)$ & $aX_1(\omega) + bX_2(\omega)$ \\
        Time shifting & $x(n-k)$ & $e^{-j\omega k}X(\omega)$ \\
        Time reversal & $x(-n)$ & $X(-\omega)$ \\
        Convolution & $x_1(n) \ast x_2(n)$ & $X_1(\omega)X_2(\omega)$ \\
        Parseval's theorem & $\sum_{n=-\infty}^{\infty}x(n)y^*(n)$ & $\frac{1}{2\pi}\int_{-\pi}^{\pi}X(\omega)Y^*(\omega)d\omega$ \\
        \hline
    \end{tabular}
    \caption{Properties of DTFT}
    \label{table:DTFT_properties}
\end{figure}

\subsubsection{DFT}
\paragraph{Discrete Fourier transform (DFT):} The DFT of a discrete-time signal $x[n]$ is defined as:

\begin{align}
    \text{DFT of } N\text{-length signal:} \quad & X(k) = \sum_{n=0}^{N-1} x[n]e^{-j\frac{2\pi}{N}kn} \\
    \text{Inverse DFT of } N\text{-length signal:}  \quad & x(n) = \frac{1}{N}\sum_{k=0}^{N-1} X(k)e^{j\frac{2\pi}{N}kn}
\end{align}

\subsubsection{Relationship between DFT and DTFT}

The $N$-point DFT is the spectrum of the DFTF of:
\begin{equation}
    \bar{X}(e^{j\omega}) =X(e^{j\omega}) \ast \left( \frac{\sin (\omega L / 2)}{\sin (\omega/2)}e^{-j(L-1)\omega/2} \right) 
\end{equation}
Evaluated at the $N$ points given by $\omega_k = \frac{2\pi}{N}k, \quad k=0,1,\ldots,N-1$. I.e. 
\begin{equation}
    X(k) = \bar{X}\left( \frac{2\pi}{N}k \right)
\end{equation}

This means we must have at least as many samples in the frequency domain as we do in the time domain.
\subsubsection{The four Fourier transforms}

\subsubsection{Ideal filters}
\subsubsection{Example of simple filters}
\subsubsection{Min/max/mixed phase and inverse system}
\subsubsection{Circular and linear convolution}
\subsubsection{Advantages of FFT. The computational complexity to DFT to definition}

\subsection{Filter design}

\subsubsection{Advantages/Disadvantages of FIR and IIR filters}
\paragraph{Advantages, FIR filters:}
\begin{itemize}
    \item Can be designed to have exact linear phase, which makes h(n) zero points symmetric about $|z| = 1$
    \item  Filter structures are always stable for quantized coefficients.
    \begin{itemize}
        \item Stable IIR filters can become unstable due to rounding errors in coefficients.
    \end{itemize}
    \item Design methods are generally linear. 
    \item Start transient has finite length.
    \item Can be realized efficently in HardWare (but not as efficient as IIR filters)
\end{itemize}

\paragraph{Disadvantages, FIR filters:}

Generally require a higher order than IIR filters to meet the same specifications. This often ensure the FIR filter having greater calculation burden/complexity and larger group delay than the corresponding IIR filter.

\paragraph{Advantages, IIR filters:}
\begin{itemize}
    \item In IIR filter design, the possibility of converting a analog filter to a digital filet gives them several advantages:
    \begin{itemize}
        \item Analog approximation techniques are highly advanced.
        \item They usually yield closed-form solutions.
        \item Extensice tables are available for analog filters.
        \item Many applications require digital simulations of analog system.
    \end{itemize}
\end{itemize}

\paragraph{Disadvantages, IIR filters:}
We have no control over the phase characteristics of a IIR filter, we only know it cannot have linear phase). I.e. IIR filters have only magnitude-only deigns.

\subsubsection{Common ways to design IIR filters}
\begin{itemize}
    \item Specify the filter as a analog/continuous time (CT) equivalent to the dicrete IIR filter you want to design.
    \item Design the analog lowpass filter in continuous time.
    \item Convert the analog lowpass filter to a digital filter using filter transformation.
    \item Convert the digital lowpass filter to the desired filter using a frequency band transformation.
    \item Convert the digital filter to a discrete time (DT) filter (From s-domain to z-domain).
    \item You could also just use an optimization algorithm (e.g. least-square method)
\end{itemize}

\subsubsection{Typical IIR filters}
When using the approximation method, you have the desired response given as:
\begin{equation}
    |H_d (j\Omega)|^2 = \begin{cases}
        1, & 0 \leq |\Omega| \leq \Omega_c \\
        0, & |\Omega| > \Omega_c
    \end{cases}
\end{equation}

And the approximation as:
\begin{equation}
    |H_c (j\Omega)|^2 = \frac{1}{1 + V^2(\Omega)}
\end{equation}
Where $V^2(\Omega) << 1$ for $|\Omega| \leq \Omega_c$ and $V^2(\Omega) >> 1$ for $|\Omega| > \Omega_c$.

The $V^2(\Omega)$ is called the \textbf{approximation function} and is used to characterize the approximation. Different $V^2(\Omega)$ gives different approximations types.

\paragraph{Butterworth filter:}
\begin{itemize}
    \item $ |H(\Omega)|^2 = \frac{1}{1+\left(\frac{\Omega}{\Omega_c}\right)^{2N}}$, where $N$ is the filter order.
    \item Is flat in the passband, and rolls off towards zero in the stopband.
    \item In a logarithmic plot (bode plot), the response slopes off linearly towards $-\infty$.
    \item Butterworth filters have a monotonically changing magnitude response with frequency $\omega$.
\end{itemize}

\paragraph{Chebyshev filter:}
\begin{itemize}
    \item $|H(\Omega)|^2 = \frac{1}{1 + \epsilon^2 T_N^2 (\Omega / \Omega_c)}$, where $T_N$ is the Nth order Chebyshev polynomial ().
    \item The Chebyshev filter has a ripple in either the passband or stopband, but a steeper roll-off than the Butterworth filter.
    \item Type I Chebyshev filter is an all pole filter and has equiripple (the ripples are of same height) in the passband and a monotonically decreasing stopband.
    \item Type II Chebyshev filter consists of both poles and zeros, with a monotonically decreasing passband and equiripple in the stopband.
\end{itemize}

\paragraph{Elliptic filter:}
\begin{itemize}
    \item $|H(\Omega)|^2 = \frac{1}{1 + \epsilon^2 U_N^2 (\Omega / \Omega_c ) }$, where $U_N(\Omega / \Omega_c)$ is a Jacobian elliptic function.
    \item The elliptic filter has a ripple in both the passband and stopband.
    \item The elliptic filter has both poles and zeros.
    \item Compared to the two other types, the elliptic filter has the narrowest transition band for a given $N$, cutoff frequency $\Omega_p$ and pass/stop-band ripples.
\end{itemize}

\paragraph{A small summary so far:}
Of the prototype analog filters showcased here, \textbf{Elliptic gives best performance in the magnitude-squared response}, but have a highly nonlinear phase response in the passband (Often bad in application). 

\textbf{Butterworth filters} are on the other end with low performance in the magnitude-squared response, but have \textbf{flat magnitude response in the passband and a fairly linear phase in the passband}. They do however require more poles ($N$) to achieve the same stopband specification.


\subsubsection{Common ways to design FIR filters}
FIR filter design is often based on a approximation of a specified magnitude response. The design of a FIR filter of order $M-1$ can be found by either the length $M$ impulse response samples in $\{ h(n) \}$ or the $M$ samples of the frequency response in $\{ H(e^{j\omega}) \}$.
\begin{itemize}
    \item \textbf{Window method:} Multiply the ideal impulse response with a window function to get the desired impulse response.
    \item \textbf{Frequency sampling method:} Sample the frequeny response (DTFT) of the desired filter by assuming it to be the DFT. Then use the inverse DFT to get the desired coefficients. This can work well for in the points sampled, but between the points the frequency response can not be controlled. Transition samples from tables can improve this.
    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.75\textwidth]{figures/Filter design/frequency_sampling_method.png}
        \caption{Visual representation of the frequency sampling method}
        \label{fig:freq_sampling_method}
    \end{figure}
    
    \item \textbf{Optimization method:} Use algorithms (e.g. Parks-McClellan) or optimization techniques like least squares to iteratively refine the filter coefficients by minimizing the error between the desired and actual frequency response.
\end{itemize}

\subsubsection{Window functions and the consequences of applying them}
Window functions $w(n)$ are of finite lengths and symmetric around their midpoints. 
\begin{equation}
    \underbrace{h(n)}_{\text{Desired filter}} = \underbrace{h_d (n)}_{\text{Ideal filter}} \ast \ w(n)
\end{equation}
Or in the frequency domain:
\begin{equation}
    H(e^{j\omega}) =  H_d (e^{j\omega}) \circledast W(e^{j\omega}) = \frac{1}{2\pi} \int_{-\pi}^{\pi} H_d (e^{j\nu}) W(e^{j(\omega - \nu)}) d\nu
\end{equation}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/Filter design/lowpass_tolerance_diagram.png}
    \caption{Example of a lowpass filter tolerance diagram}
    \label{fig:tolerance_diagram}
\end{figure}

How well $H(e^{j\omega})$ approximates $H_d (e^{j\omega})$ is determined by:
\begin{itemize}
    \item The width of the main lobe of $W(e^{j\omega})$
    \item Maximum side lobe level of $W(e^{j\omega})$
\end{itemize}

This method also lacks precise control of $w_p$ and $w_s$.

\paragraph{Some characteristics of window spectrum:}
\begin{itemize}
    \item Pass and stopband ripples are not constant
    \item Stopband level (filter) typically less than peak sidelobe level (PSL) to window. 
    \item PSL, peak passband ripple and passband attenuation $\approx$ independent of $N$.
    \item Width of transition band $\approx$ given by main lobe width window, $F_{WS} \approx C/N$ where $C$ is a constant for each window.
\end{itemize}

\subsubsection{Linear phase and linear-phase filters}
A LTI system has a \textbf{linear phase} if:
\begin{equation}
    H(e^{j\omega}) = |H(e^{j\omega})|e^{-j\alpha\omega}
\end{equation}
Or a \textbf{generalized linear phase} if:
\begin{equation}
    H(e^{j\omega}) = A(e^{j\omega})e^{-j(\alpha\omega - \beta)}
\end{equation}
Where $A(e^{j\omega})$ is a real-valued function of $\omega$ and $\beta \in \mathfrak{R} $
\\
\\
If your system needs to both be \textbf{causal} and have \textbf{linear phase}, you can only use \textbf{FIR}.
\\
\\
A sufficient condition for a real-valued FIR filter to have generalized linear phase is that its impulse response $h(n)$ is symmetric or antisymmetric.
\begin{itemize}
    \item \textbf{Symmetric:} $h(n) = h(M-1-n), \quad n=0,1,\ldots,M-1$
    \\
    Then $\alpha = (M-1)/2$ and $\beta = 0 \ \vee \ \pi$
    \item \textbf{Antisymmetric:} $h(n) = -h(M-1-n), \quad n=0,1,\ldots,M-1$
    \\
    Then $\alpha = (M-1)/2$ and $\beta = \pi/2 \ \vee \ 3\pi/2$
\end{itemize}
For a real linear phase FIR filter with a zero $z_1$, you also have zeros at $z_1^\ast, \frac{1}{z_1}$ and $\frac{1}{z_1^\ast}$
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.4\textwidth]{figures/Filter design/zero_location_FIR.png}
    \caption{Example showing zero symmetry in a FIR filter}
    \label{fig:zero_location_FIR}
\end{figure}

\clearpage

\subsubsection{Graphical representation of filter structures and operations}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/Filter design/structure_forms.png}
    \caption{Example of a filter structure}
    \label{fig:filter_structure}
\end{figure}

\paragraph{Operator notation:} The $z^{-1}$ blocks in fig.~\ref{fig:filter_structure} are called \textbf{delay operators} and are used to represent the delay of one sample. I.e. $x[n-1] \begin{smallmatrix} \mathcal{Z} \\ \longleftrightarrow \end{smallmatrix} z^{-1}X(z)$

\paragraph{How is it made} Given a transfer function: \[H(z) = \frac{Y(z)}{X(z)} = (b_0 + b_1z^{-1} \ldots) \left( \frac{1}{1-a_1z^{-1} \ldots} \right) = B(z)\frac{1}{A(z)}\]
Note we have two forms, \textbf{direct form I} and \textbf{direct form II}. The later saves one unit delay.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/Filter design/connected_systems.png}
    \caption{Illustration of connected systems}
    \label{fig:connected_systems}
\end{figure}
Figure~\ref{fig:connected_systems} shows how a equivalent transfer function of systems in cascade is the product of the individual transfer functions. The equivalent transfer function of systems in parallel is the sum of the individual transfer functions.

\subsubsection{Gibb's phenomenon}

\begin{itemize}
    \item Oscillatory behavior in the magnitude response of causal FIR filters found by truncating the filter coefficients of an ideal filter.
    \item If you increase the length of the filter, the number of ripple-peaks will increase in both passband and stopband and the width of these peaks decrease equivalent.
    \item Given the same weighting function, the highest ripple-peak will be unchanged. 
\end{itemize}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/Filter design/gibbs_effekt.png}
    \caption{Example of Gibb's effect in a lowpass filter}
    \label{fig:gibbs_effect}
\end{figure}

\clearpage

\subsection{Sampling and reconstruction}
\subsubsection{Sampling theorem}
The sampling frequency $\Omega_T = 2\Omega_H$ is called the Nyquist frequency. If this term is satisfied, then the original signal can be recovered from the sampled signal. In practice we often use oversampling (sampling at a higher rate than the Nyquist frequency) to get an appropiate reconstruction.
\subsubsection{Synthesizing an arbitrary signal as a sum of unit impulses}

Sampling corresponds to multiplying with a Dirac comb with distance $T$ between the unit impulses.
In the Fourier domain, this corresponds to convolving with a Dirac comb with distance $F_T$ between
scaled unit impulses. (The distance is $\Omega_T$ if we look at the angular frequency axis)
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{figures/Sampling and reconstruction/Sampling_dirac_impulses.png}
    \caption{Sampling with dirac impulses}
    \label{fig:sampling_dirac_impulses}
\end{figure}
\begin{equation}
    x_p (t) = x_a (t) III_T (t) = \sum_{n=-\infty}^{\infty} x_a (t) \delta (t - nT) = \sum_{n=-\infty}^{\infty} x_a (nT) \delta (t - nT) \triangleq x(n)
\end{equation}

\clearpage
\subsubsection{Sampling of band-limited signals}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\textwidth]{figures/Sampling and reconstruction/sampling_band_lim_signal.png}
    \caption{Periodic sampling in freq. domain}
    \label{fig:sampling_band_lim_signal}
\end{figure}

\paragraph{Important part:} A signal that is limited to a band $F \in [F_L,F_H]$ is a bandpass with bandwidth $B = F_H - F_L$. This signal can be perfectly reconstructed if $2B \le F_T \le 4B$

\paragraph{Not important}The reconstructed signal will repeat itself in the frequency domain. As long the sampling frequency ($\Omega_T$)is equal or greater than the Nyquist rate ($2\Omega_H$), we will be able to seperate the original signal from the aliases. The reconstruction in fig. \ref{fig:sampling_band_lim_signal} can be written as a convolution:
\begin{equation}
    x_r (t) = \sum_{n=-\infty}^{\infty} x_a (nT) g_r (t - nT) 
\end{equation}

Where $g_r (t)$ is the interpolation reconstruction function (I.e. impulse response). 

\subsubsection{Ideal reconstruction}

The fundamental copy of the Fourier-domain representation of the sampled signal can be found by the 'ideal' filter for the convolution operation is: 
\begin{equation}
    G_r (j\Omega) = \bigg\{ \begin{matrix}
        T, & |\Omega| \leq \Omega_T /2 \\
        0, & |\Omega| > \Omega_T / 2
    \end{matrix}
\end{equation}

This gives the ideal: $X_r (j\Omega) = X_a (j\Omega)$ \\ (Reconstructed Fourier domain representation equals the original Fourier domain representation).

\clearpage
\subsubsection{Upsampling and downsampling}

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{figures/Sampling and reconstruction/sampling_examples.png}
    \caption{Example of over/undersampling}
    \label{fig:sampling_example}
\end{figure}

\section{Additional Topics}
\subsection{Discrete time}
\subsubsection{Symmetrical signals}

\subsection{LTI systems and characteristics}

\subsection{Convolution and correlation}

\subsection{Z-transform}

\subsection{Frequency analysis and DFT}

\subsection{Filter design}

\subsection{Sampling and reconstruction}

\paragraph{Notes from group session:}
\begin{itemize}
    \item x(n) is input, y(n) is ouput and h(n) is impulse response. 
    \item FIR filter has to have symmetrical coefficients to have linear phase.
    \item The placement of the zero can give us the magnitude plot. And from the magnitude plot we can see the frequency spectrum. 
    \item Three ways to see if it is causal, ROC or difference equation
    \item From ROC, it is causal if it contains infinity
    \item For n<0, if h(n) (Impulse response) have a postive value, it it anti-causal.
\end{itemize}
\end{document}